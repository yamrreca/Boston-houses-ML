{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "* Write a class for preprocessing in a pipeline:\n",
    "    * Add the option to remove the NOX variable.\n",
    "    * Apply logarithm to PTRATIO, INDUS, and TAX.\n",
    "* Use only top predictors (corr >= 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pprint import pprint\n",
    "from math import ceil\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_outliers_below(df, columns):\n",
    "    \"\"\"Returns its input dataframe without the lower outliers of the features given in the columns iterable.\"\"\"\n",
    "    df2 = df.copy()\n",
    "    if type(df) == 'pandas.core.frame.DataFrame':\n",
    "        for column in columns:\n",
    "            Q1 = df2[column].quantile(0.25)\n",
    "            Q3 = df2[column].quantile(0.75)\n",
    "            mult = 1.5*(Q3-Q1)\n",
    "            outlier_ix = df2[df2[column] <= Q1 - mult].index\n",
    "            df2.drop(index=outlier_ix, inplace = True)\n",
    "    elif type(df) == 'pandas.core.series.Series':\n",
    "        Q1 = df2.quantile(0.25)\n",
    "        Q3 = df2.quantile(0.75)\n",
    "        mult = 1.5*(Q3-Q1)\n",
    "        outlier_ix = df2[df2 <= Q1 - mult].index\n",
    "        df2.drop(index=outlier_ix, inplace = True)\n",
    "        \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_outliers_above(df, columns):\n",
    "    \"\"\"Returns its input dataframe without the upper outliers of the features given in the columns iterable.\"\"\"\n",
    "    df2 = df.copy()\n",
    "    for column in columns:\n",
    "        if type(df) == 'pandas.core.frame.DataFrame':\n",
    "            Q1 = df2[column].quantile(0.25)\n",
    "            Q3 = df2[column].quantile(0.75)\n",
    "            mult = 1.5*(Q3-Q1)\n",
    "            outlier_ix = df2[df2[column] >= Q3 + mult].index\n",
    "            df2.drop(index=outlier_ix, inplace = True)\n",
    "        elif type(df) == 'pandas.core.series.Series':\n",
    "            Q1 = df2.quantile(0.25)\n",
    "            Q3 = df2.quantile(0.75)\n",
    "            mult = 1.5*(Q3-Q1)\n",
    "            outlier_ix = df2[df2 >= Q3 + mult].index\n",
    "            df2.drop(index=outlier_ix, inplace = True)\n",
    "        \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_skew(df):\n",
    "    \"\"\"Applies the logarithm to the columns with a skewness greater than 0.5 (not absolute value).\"\"\"\n",
    "    df2 = df.copy()\n",
    "    if type(df) == 'pandas.core.frame.DataFrame':\n",
    "        for column in df.columns:\n",
    "            if df2[column].skew() >= 0.5:\n",
    "                df2[column] = np.log(df2[column])\n",
    "    elif type(df) == 'pandas.core.series.Series':\n",
    "        if df2.skew() >= 0.5:\n",
    "                df2 = np.log(df2)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, poly_features=None, max_degree=3, rm_outliers=None):\n",
    "        self.poly_features = poly_features #List of features to make polynomial\n",
    "        self.max_degree = max_degree #Maximum polynomial degree\n",
    "        self.rm_outliers = rm_outliers #Features to remove upper outliers from\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_temp = X.copy()\n",
    "        #Remove outliers\n",
    "        if self.rm_outliers is not None:\n",
    "            X_temp = rm_outliers_above(X_temp, self.rm_outliers)\n",
    "        #Apply log to reduce skew\n",
    "        X_temp = reduce_skew(X_temp)\n",
    "        #Make polynomial features\n",
    "        if self.poly_features is not None:\n",
    "            poly_feat_transformer = PolynomialFeatures(degree=self.max_degree)\n",
    "            X_temp = poly_feat_transformer.fit_transform(X_temp[self.poly_features])\n",
    "            \n",
    "        return X_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_grid_search_eval( X_train, y_train, pipe, param_grid):\n",
    "    \"\"\"Runs the given datasets through a pipeline, performs a grid search and evaluates its performance through\n",
    "    cross-validation and mean squared error.\n",
    "    \n",
    "    Arguments:\n",
    "    -pipeline: Sklearn pipeline object. An ML model must be trained at some step in the pipe.\n",
    "    -param_grid: List of dictionaries with the hyperparamaters to try in the grid search.\n",
    "    -X_train: NumPy array with the features to be trained on.\n",
    "    -y_train: NumPy array with the labels used to train and evaluate.\n",
    "    \n",
    "    Returns:\n",
    "    search_res: The fit GridSearchCV\n",
    "    \"\"\"\n",
    "    grid_search = GridSearchCV(pipe, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    search_res = grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_cv_score = np.sqrt(-search_res.best_score_)\n",
    "    best_model = search_res.best_estimator_\n",
    "    best_train_score = np.sqrt(mean_squared_error(y_train, best_model.predict(X_train)))\n",
    "    \n",
    "    pprint('Best paramaters:\\n' + str(search_res.best_params_))\n",
    "    print('\\n')\n",
    "    print('''CV error: %.2f\n",
    "train error: %.2f\n",
    "Label mean for comparison: %.2f'''%(best_cv_score, best_train_score, y_train.mean()))\n",
    "    \n",
    "    return search_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_test = pd.read_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll apply a simple first model to get an idea of how it well it performs. I'll use all features with a correlation coefficient with the target greater than 0.5, except for the RAD variable due to its extremely high correlation with TAX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = pd.merge(X_train, y_train, left_index = True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers from y2_train\n",
    "#X2 = rm_outliers_above(X2, ['target'])\n",
    "outlier_ix = X2[X2['target'] >= 50].index\n",
    "X2.drop(index=outlier_ix, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE', 'CRIM']\n",
    "X2_train = X2[predictors].copy()\n",
    "y2_train = X2['target'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feat_transformer = FeatureTransformer()\n",
    "standard_scaler = StandardScaler()\n",
    "l_reg = LinearRegression()\n",
    "pipe = make_pipeline(X_feat_transformer,\n",
    "    standard_scaler,\n",
    "    l_reg\n",
    ")\n",
    "#X2_train_stand = scaler.fit_transform(X2_train)\n",
    "#X2_train_stand = pd.DataFrame(X2_train_stand, columns=X2_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe.fit(X2_train, y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in X2_train.columns:\n",
    "    if np.abs(X_train[col].skew()) > 0.5:\n",
    "        X2_train[col] = np.log1p(X2_train[col])\n",
    "X2_train = standard_scaler.fit_transform(X2_train)\n",
    "y2_train = np.log1p(y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = cross_val_score(l_reg, X2_train, y2_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_score = np.sqrt(-cv_scores.mean())\n",
    "cv_std = cv_scores.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_reg.fit(X2_train, y2_train)\n",
    "train_score = np.sqrt(mean_squared_error(y2_train, l_reg.predict(X2_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.19 +/- 0.00\n",
      "train score: 0.18\n",
      "Label mean for comparison: 22.80\n"
     ]
    }
   ],
   "source": [
    "print('CV score: %.2f +/- %.2f\\ntrain score: %.2f\\nLabel mean for comparison: %.2f'%(cv_score, cv_std, train_score, y_train.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the training set error is smaller than the CV error, the model is overfitting. I'll try to engineer a couple of features next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll perform a quick grid search to figure out if it's worth it to apply a polynomial transformation to the LSTAT variable. Also, I'll try several combinations of features to apply a log to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_corr_features = list(np.abs(corr_matrix['target']).sort_values().index[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_features = [predictors]\n",
    "#drop_features= [None, \n",
    " #               ['NOX'],\n",
    "  #              ['NOX', 'TAX', 'LSTAT']]\n",
    "param_grid = [\n",
    "    {'feature_transformer__poly_features':poly_features,\n",
    "     'feature_transformer__max_degree':list(range(2,5))\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('feature_transformer', FeatureTransformer()),\n",
    "    ('standardization', StandardScaler()),\n",
    "    ('linear_reg', LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best paramaters:\\n'\n",
      " \"{'feature_transformer__max_degree': 2, 'feature_transformer__poly_features': \"\n",
      " \"['LSTAT', 'RM', 'PTRATIO', 'INDUS', 'TAX']}\")\n",
      "\n",
      "\n",
      "CV error: 3.24\n",
      "train error: 2.88\n",
      "Label mean for comparison: 20.70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('feature_transformer',\n",
       "                                        FeatureTransformer()),\n",
       "                                       ('standardization', StandardScaler()),\n",
       "                                       ('linear_reg', LinearRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'feature_transformer__max_degree': [2, 3, 4],\n",
       "                          'feature_transformer__poly_features': [['LSTAT', 'RM',\n",
       "                                                                  'PTRATIO',\n",
       "                                                                  'INDUS',\n",
       "                                                                  'TAX']]}],\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_grid_search_eval(X2_train, y_train, pipe, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new polynomial and logarithmic features, the train score diminished considerably, and the CV score also went down a little. However, the model is now overfitting, so I'll try regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_param_grid = [\n",
    "    {'feature_transformer__poly_features':poly_features,\n",
    "     'feature_transformer__max_degree':list(range(4,6)),\n",
    "     'ridge_reg__alpha':np.arange(9,15)}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_pipe = Pipeline([\n",
    "    ('feature_transformer', FeatureTransformer()),\n",
    "    ('standardization', StandardScaler()),\n",
    "    ('ridge_reg', Ridge())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best paramaters:\\n'\n",
      " \"{'feature_transformer__max_degree': 4, 'feature_transformer__poly_features': \"\n",
      " \"['LSTAT', 'RM', 'PTRATIO', 'INDUS', 'TAX'], 'ridge_reg__alpha': 14}\")\n",
      "\n",
      "\n",
      "CV error: 3.19\n",
      "train error: 2.91\n",
      "Label mean for comparison: 20.70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('feature_transformer',\n",
       "                                        FeatureTransformer()),\n",
       "                                       ('standardization', StandardScaler()),\n",
       "                                       ('ridge_reg', Ridge())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid=[{'feature_transformer__max_degree': [4, 5],\n",
       "                          'feature_transformer__poly_features': [['LSTAT', 'RM',\n",
       "                                                                  'PTRATIO',\n",
       "                                                                  'INDUS',\n",
       "                                                                  'TAX']],\n",
       "                          'ridge_reg__alpha': array([ 9, 10, 11, 12, 13, 14])}],\n",
       "             scoring='neg_mean_squared_error')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_grid_search_eval(X2_train, y_train, reg_pipe, reg_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still overfitting. Perhaps trying a less complicated model will fix it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_pipe = Pipeline([\n",
    "    ('feature_transformer', FeatureTransformer()),\n",
    "    ('standardization', StandardScaler()),\n",
    "    ('SVR', SVR())\n",
    "])\n",
    "\n",
    "#Defining a new parameter grid for the SVR\n",
    "svr_param_grid = [\n",
    "    {'feature_transformer__poly_features':poly_features,\n",
    "     'feature_transformer__max_degree':list(range(4,6)),\n",
    "     'SVR__gamma':np.arange(1,11),\n",
    "     'SVR__C':np.arange(10,21),\n",
    "     'SVR__kernel':['linear','rbf']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Best paramaters:\\n{'SVR__C': 11, 'SVR__gamma': 1, 'SVR__kernel': 'rbf'}\"\n",
      "\n",
      "\n",
      "CV error: 3.49\n",
      "train error: 2.29\n",
      "Label mean for comparison: 20.70\n"
     ]
    }
   ],
   "source": [
    "search_res = pipeline_grid_search_eval( X2_train, y_train, svr_pipe, svr_param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one performs actively worse than the linear model, though at least it doesn't overfit as badly. Of note is also that with this model the set of features to apply the log to is different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_pipe = Pipeline([\n",
    "    ('feature_transformer', FeatureTransformer()),\n",
    "    ('standardization', StandardScaler()),\n",
    "    ('random_forest', RandomForestRegressor(n_estimators=200,max_features='sqrt',random_state=42, n_jobs=-1))\n",
    "])\n",
    "\n",
    "#Defining a new parameter grid for the SVR\n",
    "forest_param_grid = [\n",
    "    {'feature_transformer__poly_features':poly_features,\n",
    "     'feature_transformer__max_degree':list(range(2,5)),\n",
    "     #'random_forest__n_estimators': np.arange(100,301,100),\n",
    "     'random_forest__max_depth':np.arange(10,15),\n",
    "     #'random_forest__max_leaf_nodes':np.arange(95, 100),\n",
    "     'random_forest__max_features':['auto', 'sqrt', 'log2']\n",
    "     }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Best paramaters:\\n'\n",
      " \"{'feature_transformer__max_degree': 2, 'feature_transformer__poly_features': \"\n",
      " \"['LSTAT', 'RM', 'PTRATIO', 'INDUS', 'TAX'], 'random_forest__max_depth': 10, \"\n",
      " \"'random_forest__max_features': 'sqrt'}\")\n",
      "\n",
      "\n",
      "CV error: 3.16\n",
      "train error: 1.27\n",
      "Label mean for comparison: 20.70\n"
     ]
    }
   ],
   "source": [
    "search_res = pipeline_grid_search_eval( X2_train, y_train, forest_pipe, forest_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
